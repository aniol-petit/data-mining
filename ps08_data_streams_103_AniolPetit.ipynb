{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "<font size=\"+2\" color=\"blue\">Additional results: Heap's law</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Aniol Petit Cabarrocas</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">aniol.petit01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">20/11/2024</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=20000, report_every=5000):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print(\"Current noun '%s'\" % (word)) \n",
    "end_time = time.time()\n",
    "print(f\"Total time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    if len(reservoir) < max_reservoir_size:\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        index = random.randint(0, len(reservoir))\n",
    "        if index < max_reservoir_size:\n",
    "            reservoir[index] = item\n",
    "    assert len(reservoir) <= max_reservoir_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(filename, parts_of_speech, max_words=max_words, report_every=report_every):\n",
    "        words_read += 1\n",
    "\n",
    "        if max_words != -1 and words_read > max_words:\n",
    "            break\n",
    "\n",
    "        if len(reservoir) < reservoir_size:\n",
    "            add_to_reservoir(reservoir, word, reservoir_size)\n",
    "        else:\n",
    "            if random.random() < reservoir_size / words_read:\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "\n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "total_items = len(reservoir)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    relative_frequency = (absolute_frequency / total_items) * 100  \n",
    "    print(\"%d %s (%.2f%%)\" % (absolute_frequency, word, relative_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes about 2-3 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_sizes = [50, 100, 500, 1000, 1500]\n",
    "max_words = 20000  \n",
    "\n",
    "dataset_size = max_words  \n",
    "\n",
    "for reservoir_size in reservoir_sizes:\n",
    "    print(f\"\\nReservoir Size: {reservoir_size}\")\n",
    "    \n",
    "    items_seen, reservoir = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=max_words, report_every=20000)\n",
    "    \n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = freq.get(item, 0) + 1\n",
    "\n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "    \n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        estimated_absolute_frequency = absolute_frequency * dataset_size / reservoir_size\n",
    "        estimated_relative_frequency = (estimated_absolute_frequency / dataset_size) * 100\n",
    "        print(f\"{word}: Estimated Absolute Frequency = {estimated_absolute_frequency:.0f}, \"\n",
    "              f\"Estimated Relative Frequency = {estimated_relative_frequency:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_sizes = [50, 100, 500, 1000, 1500]\n",
    "max_words = 20000  \n",
    "\n",
    "dataset_size = max_words\n",
    "\n",
    "for reservoir_size in reservoir_sizes:\n",
    "    print(f\"\\nReservoir Size: {reservoir_size}\")\n",
    "    \n",
    "    items_seen, reservoir = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=max_words, report_every=20000)\n",
    "    \n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = freq.get(item, 0) + 1\n",
    "\n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "    \n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        estimated_absolute_frequency = absolute_frequency * dataset_size / reservoir_size\n",
    "        estimated_relative_frequency = (estimated_absolute_frequency / dataset_size) * 100\n",
    "        print(f\"{word}: Estimated Absolute Frequency = {estimated_absolute_frequency:.0f}, \"\n",
    "              f\"Estimated Relative Frequency = {estimated_relative_frequency:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small sizes (50, 100) we get inconsistent results between runs, with size 500 there is some more consistency but there are still some fluctuations. For larger sizes (1000, 1500) we see more stable results, getting in 2 consecutive runs the same top-3 words (and in 1 case, size 100, same top-5 with a slight difference in frequencies). \n",
    "\n",
    "Given the above observations, a reservoir size of at least 1000 is recommended as these sizes yield more stable results, with consistent top words across trials, which seems that as we increase the size we get more stable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    \n",
    "    max_trailing_zeroes = 0\n",
    "    \n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=20000, report_every=5000):\n",
    "        hash_value = hash_function(word) \n",
    "        trailing_zeroes = count_trailing_zeroes(hash_value) \n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes) \n",
    "        \n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Compute the median of average estimates in 3 separate runs of your algorithm; each run should do 10 passes over the file. Repeat this for nouns (POS_NOUN), adjectives (POS_ADJECTIVE), and verbs (POS_VERB). Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flajolet_martin_estimate(pos_tag, num_runs=3, num_passes=10):\n",
    "    all_run_estimates = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1} for POS Tag: {pos_tag}\")\n",
    "        estimates = []\n",
    "\n",
    "        for pass_idx in range(num_passes):\n",
    "            hash_function = random_hash_function()\n",
    "\n",
    "            max_trailing_zeroes = 0\n",
    "\n",
    "            for word in read_by_parts_of_speech(INPUT_FILE, [pos_tag], max_words=5000, report_every=1000):\n",
    "                hash_value = hash_function(word) \n",
    "                trailing_zeroes = count_trailing_zeroes(hash_value)  \n",
    "                max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)  \n",
    "\n",
    "            estimate = 2 ** max_trailing_zeroes\n",
    "            estimates.append(estimate)\n",
    "            print(f\"  Pass {pass_idx + 1}: Estimate = {estimate}\")\n",
    "\n",
    "        median_estimate = statistics.median(estimates)\n",
    "        print(f\"Median for Run {run + 1}: {median_estimate}\")\n",
    "        all_run_estimates.append(median_estimate)\n",
    "\n",
    "    overall_median = statistics.median(all_run_estimates)\n",
    "    print(f\"\\nOverall Median for POS {pos_tag}: {overall_median}\\n\")\n",
    "    return all_run_estimates, overall_median\n",
    "\n",
    "\n",
    "noun_results, noun_median = flajolet_martin_estimate(POS_NOUN)\n",
    "adjective_results, adjective_median = flajolet_martin_estimate(POS_ADJECTIVE)\n",
    "verb_results, verb_median = flajolet_martin_estimate(POS_VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flajolet-Martin is a probabilistic method that can produce high variance in estimates; the median mitigates this by focusing on the central tendency of the values.\n",
    "Hence, for this problem the median is more appropriate as it robustly accounts for the probabilistic nature of the algorithm and its potential variance. This is particularly evident when certain runs yield anomalously high or low estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "def heaps_law_experiment(pos_tag, max_words_list, num_passes=3):\n",
    "    results = []  # Store (max_words, distinct_words) pairs\n",
    "\n",
    "    for max_words in max_words_list:\n",
    "        estimates = []\n",
    "\n",
    "        for _ in range(num_passes):\n",
    "            hash_function = random_hash_function()  # Generate a new hash function\n",
    "            max_trailing_zeroes = 0\n",
    "\n",
    "            for word in read_by_parts_of_speech(INPUT_FILE, [pos_tag], max_words=max_words, report_every=50000):\n",
    "                hash_value = hash_function(word)  \n",
    "                trailing_zeroes = count_trailing_zeroes(hash_value)  \n",
    "                max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes) \n",
    "\n",
    "            estimate = 2 ** max_trailing_zeroes\n",
    "            estimates.append(estimate)\n",
    "\n",
    "        median_estimate = statistics.median(estimates)  \n",
    "        results.append((max_words, median_estimate))\n",
    "        print(f\"max_words: {max_words}, Distinct Words (Median Estimate): {median_estimate}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_heaps_law(results, pos_tag):\n",
    "    \"\"\"Plot the results of the Heap's Law experiment.\"\"\"\n",
    "    max_words, distinct_words = zip(*results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_words, distinct_words, marker='o', label=pos_tag)\n",
    "    plt.xlabel(\"Total Words Read\")\n",
    "    plt.ylabel(\"Distinct Words (Estimated)\")\n",
    "    plt.title(f\"Heap's Law for {pos_tag}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "max_words_list = [1000, 2000, 4000, 8000, 16000, 32000]  # Logarithmic intervals\n",
    "num_passes = 3  \n",
    "\n",
    "print(\"Processing nouns...\")\n",
    "noun_results = heaps_law_experiment(POS_NOUN, max_words_list, num_passes=num_passes)\n",
    "\n",
    "print(\"Processing adjectives...\")\n",
    "adjective_results = heaps_law_experiment(POS_ADJECTIVE, max_words_list, num_passes=num_passes)\n",
    "\n",
    "print(\"Processing verbs...\")\n",
    "verb_results = heaps_law_experiment(POS_VERB, max_words_list, num_passes=num_passes)\n",
    "\n",
    "plot_heaps_law(noun_results, \"Nouns\")\n",
    "plot_heaps_law(adjective_results, \"Adjectives\")\n",
    "plot_heaps_law(verb_results, \"Verbs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
